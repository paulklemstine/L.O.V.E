import os
import json
import subprocess
import tempfile
from typing import Dict, Any

class ModelPerformanceTracker:
    def __init__(self, metrics_file: str = "model_performance.json"):
        self.metrics_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), metrics_file)
        self.metrics = self._load_metrics()

    def _load_metrics(self) -> Dict[str, Any]:
        if not os.path.exists(self.metrics_file):
            return {}
        try:
            with open(self.metrics_file, 'r') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {}

    def _save_metrics(self):
        try:
            with open(self.metrics_file, 'w') as f:
                json.dump(self.metrics, f, indent=2)
        except IOError as e:
            print(f"Error saving model metrics: {e}")

    def record_execution(self, model_name: str, tool_name: str, success: bool):
        if model_name not in self.metrics:
            self.metrics[model_name] = {}
        
        if tool_name not in self.metrics[model_name]:
            self.metrics[model_name][tool_name] = {"successes": 0, "failures": 0, "total": 0}
        
        stats = self.metrics[model_name][tool_name]
        stats["total"] += 1
        if success:
            stats["successes"] += 1
        else:
            stats["failures"] += 1
        
        self._save_metrics()

    def get_reliability(self, model_name: str, tool_name: str) -> float:
        stats = self.metrics.get(model_name, {}).get(tool_name)
        if not stats or stats["total"] == 0:
            return 1.0 # Default to confident if no data
        return stats["successes"] / stats["total"]

class AutomatedBenchmarker:
    def run_experiment(self, experiment_plan: dict, new_code: str) -> bool:
        """
        Runs the experiment in a temporary directory to validate the hypothesis.

        Args:
            experiment_plan: The plan designed by the ExperimentPlanner.
            new_code: The new code generated by the CodeGenerationAgent.

        Returns:
            True if the hypothesis is validated, False otherwise.
        """
        print("AutomatedBenchmarker: Running experiment in a temporary environment...")

        with tempfile.TemporaryDirectory() as sandbox_dir:
            try:
                test_file_path = os.path.join(sandbox_dir, "test_variant.py")
                with open(test_file_path, "w") as f:
                    f.write(new_code)

                # The generated code now prints JSON directly from the benchmark function
                command = [
                    "python", "-c",
                    "import test_variant; test_variant.run_benchmark()"
                ]

                process = subprocess.run(
                    command,
                    cwd=sandbox_dir,
                    capture_output=True,
                    text=True,
                    timeout=60
                )

                output = process.stdout.strip()

                if process.returncode != 0:
                    print(f"AutomatedBenchmarker: Experiment script failed to run. Error: {process.stderr}")
                    return False

                try:
                    results = json.loads(output)
                    old_tokens = results.get("old_tool_tokens", 0)
                    new_tokens = results.get("new_tool_tokens", 0)
                except json.JSONDecodeError as e:
                    print(f"AutomatedBenchmarker: Could not parse benchmark results. Error: {e}, Raw Output: '{output}'")
                    return False

                is_validated = new_tokens > 0 and old_tokens > 0 and new_tokens < old_tokens * 0.5

                print(f"AutomatedBenchmarker: Old tokens: {old_tokens}, New tokens: {new_tokens}")
                print(f"AutomatedBenchmarker: Hypothesis validated: {is_validated}")
                return is_validated

            except Exception as e:
                print(f"AutomatedBenchmarker: An unexpected error occurred during the experiment: {e}")
                return False