import pytest
from unittest.mock import AsyncMock, MagicMock, patch

# Mock the specialist agents before they are imported by the Orchestrator
from core.agents.specialist_agent import SpecialistAgent

class MockSpecialist(SpecialistAgent):
    def __init__(self, name, result_to_return):
        self.name = name
        self.result = result_to_return
        # Create an async mock for the method to be called
        self.execute_task_mock = AsyncMock(return_value=self.result)

    async def execute_task(self, task_details: dict) -> dict:
        # This implementation calls the mock, allowing assertions on it
        return await self.execute_task_mock(task_details)

# Import the Orchestrator after setting up the base mocks
from core.agents.orchestrator import Orchestrator

@pytest.fixture
def supervisor(mocker):
    """
    Fixture to create a fresh Orchestrator instance and then patch its
    instance-level specialist registry with mocks.
    """
    orchestrator_instance = Orchestrator()

    # Create mock specialists for the test session
    mock_analyst = MockSpecialist("Analyst", {"status": "success", "result": "Found an inefficiency."})
    mock_codegen = MockSpecialist("CodeGen", {"status": "success", "result": "Generated code successfully."})

    # Patch the INSTANCE's registry
    mocker.patch.object(orchestrator_instance, 'specialist_registry', {
        "AnalystAgent": MagicMock(return_value=mock_analyst),
        "CodeGenerationAgent": MagicMock(return_value=mock_codegen),
    })

    # Attach mocks to the instance for easy access in tests
    orchestrator_instance.mock_analyst = mock_analyst
    orchestrator_instance.mock_codegen = mock_codegen

    return orchestrator_instance

@pytest.fixture
def mock_run_llm(mocker):
    """
    Fixture to mock the global run_llm function where it is USED by the Orchestrator.
    This is the correct and most robust way to patch in this scenario.
    """
    mock = AsyncMock()
    # The Orchestrator module imports and uses run_llm, so we must patch it there.
    mocker.patch('core.agents.orchestrator.run_llm', new=mock)
    return mock


@pytest.mark.asyncio
async def test_supervisor_generates_and_executes_plan(supervisor, mock_run_llm):
    """
    Tests the end-to-end flow: a goal is given, a plan is generated by a mocked LLM,
    and the plan is executed by calling the mocked specialist agents in order.
    """
    # Arrange
    goal = "Analyze logs and generate a fix."
    plan_json = """
    ```json
    [
      {
        "specialist_agent": "AnalystAgent",
        "task_details": { "log_file": "/var/log/syslog" }
      },
      {
        "specialist_agent": "CodeGenerationAgent",
        "task_details": { "hypothesis": "{{step_1_result}}" }
      }
    ]
    ```
    """
    mock_run_llm.return_value = plan_json

    # Act
    final_result = await supervisor.execute_goal(goal)

    # Assert
    assert mock_run_llm.call_count == 1

    analyst_mock = supervisor.mock_analyst
    analyst_mock.execute_task_mock.assert_called_once_with({"log_file": "/var/log/syslog"})

    codegen_mock = supervisor.mock_codegen
    codegen_mock.execute_task_mock.assert_called_once_with({"hypothesis": "Found an inefficiency."})

    assert final_result == "Generated code successfully."

@pytest.mark.asyncio
async def test_supervisor_handles_plan_generation_failure(supervisor, mock_run_llm):
    """
    Tests that the supervisor gracefully handles a failure from the LLM
    during the plan generation phase.
    """
    # Arrange
    goal = "A goal that will fail."
    mock_run_llm.return_value = "I am sorry, I cannot create a plan for that."

    # Act
    result = await supervisor.execute_goal(goal)

    # Assert
    assert "could not generate a valid plan" in result
    supervisor.mock_analyst.execute_task_mock.assert_not_called()
    supervisor.mock_codegen.execute_task_mock.assert_not_called()

@pytest.mark.asyncio
async def test_supervisor_handles_mid_plan_execution_failure(supervisor, mock_run_llm):
    """
    Tests that the supervisor correctly halts execution and reports an error if
    one of the specialist agents returns a 'failure' status.
    """
    # Arrange
    goal = "A goal where the first step fails."
    plan_json = """
    ```json
    [
      {
        "specialist_agent": "AnalystAgent",
        "task_details": { "task": "analyze" }
      },
      {
        "specialist_agent": "CodeGenerationAgent",
        "task_details": { "task": "generate" }
      }
    ]
    ```
    """
    mock_run_llm.return_value = plan_json

    # Configure the first agent to fail
    failed_result = {"status": "failure", "result": "Could not access the log file."}
    supervisor.mock_analyst.execute_task_mock.return_value = failed_result

    # Act
    result = await supervisor.execute_goal(goal)

    # Assert
    assert "Plan execution failed" in result
    assert "Could not access the log file" in result

    supervisor.mock_analyst.execute_task_mock.assert_called_once()
    supervisor.mock_codegen.execute_task_mock.assert_not_called()
