import os
import json
import subprocess
import tempfile

class AutomatedBenchmarker:
    def run_experiment(self, experiment_plan: dict, new_code: str) -> bool:
        """
        Runs the experiment in a temporary directory to validate the hypothesis.

        Args:
            experiment_plan: The plan designed by the ExperimentPlanner.
            new_code: The new code generated by the CodeGenerationAgent.

        Returns:
            True if the hypothesis is validated, False otherwise.
        """
        print("AutomatedBenchmarker: Running experiment in a temporary environment...")

        with tempfile.TemporaryDirectory() as sandbox_dir:
            try:
                test_file_path = os.path.join(sandbox_dir, "test_variant.py")
                with open(test_file_path, "w") as f:
                    f.write(new_code)

                # The generated code now prints JSON directly from the benchmark function
                command = [
                    "python", "-c",
                    "import test_variant; test_variant.run_benchmark()"
                ]

                process = subprocess.run(
                    command,
                    cwd=sandbox_dir,
                    capture_output=True,
                    text=True,
                    timeout=60
                )

                output = process.stdout.strip()

                if process.returncode != 0:
                    print(f"AutomatedBenchmarker: Experiment script failed to run. Error: {process.stderr}")
                    return False

                try:
                    results = json.loads(output)
                    old_tokens = results.get("old_tool_tokens", 0)
                    new_tokens = results.get("new_tool_tokens", 0)
                except json.JSONDecodeError as e:
                    print(f"AutomatedBenchmarker: Could not parse benchmark results. Error: {e}, Raw Output: '{output}'")
                    return False

                is_validated = new_tokens > 0 and old_tokens > 0 and new_tokens < old_tokens * 0.5

                print(f"AutomatedBenchmarker: Old tokens: {old_tokens}, New tokens: {new_tokens}")
                print(f"AutomatedBenchmarker: Hypothesis validated: {is_validated}")
                return is_validated

            except Exception as e:
                print(f"AutomatedBenchmarker: An unexpected error occurred during the experiment: {e}")
                return False